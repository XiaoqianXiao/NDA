{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T21:52:18.463749Z",
     "start_time": "2025-06-03T21:52:13.671994Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c3dc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDA_dir = '/Users/xiaoqianxiao/lab/R01/NDA'\n",
    "version = 'sumission_20260115'\n",
    "meta_file_name = 'IFOCUSS_SubInfo.csv'\n",
    "sumission_dir = os.path.join(NDA_dir, version)\n",
    "result_dir = os.path.join(sumission_dir, 'filled')\n",
    "meta_file_path = os.path.join(sumission_dir, 'demo', meta_file_name)\n",
    "df_meta = pd.read_csv(meta_file_path)\n",
    "df_meta.head()\n",
    "df_meta.columns\n",
    "GUID_file_path = os.path.join(sumission_dir, 'demo', 'GUID_NoHIPinfo.csv')\n",
    "df_GUID = pd.read_csv(GUID_file_path)\n",
    "df_meta = df_meta.merge(df_GUID[['ID', 'GUID']], on='ID', how='left')\n",
    "# Create date_of_birth column in yyyy/mm/dd format from MOB, DOB, YOB\n",
    "df_meta['date_of_birth'] = df_meta.apply(\n",
    "    lambda row: f\"{int(row['YOB']):04d}/{int(row['MOB']):02d}/{int(row['DOB']):02d}\" \n",
    "    if pd.notna(row['YOB']) and pd.notna(row['MOB']) and pd.notna(row['DOB']) \n",
    "    else None, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6269d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabs found: ['ndar_subject01', 'cde_gad701', 'cde_phq901', 'cde_dsm5crossad01', 'cde_whodas01', 'scidv_rv01', 'lsas01_sr', 'lsas01_cr', 'sds01', 'qids01', 'pswq01', 'hars01', 'babs01', 'rumination01']\n"
     ]
    }
   ],
   "source": [
    "template_dir = '/Users/xiaoqianxiao/lab/R01/NDA/data_structure_templates'\n",
    "defination_dir = '/Users/xiaoqianxiao/lab/R01/NDA/data_structure_defination'\n",
    "approved_dir = os.path.join(template_dir, 'approved')\n",
    "redcap_file_dir = os.path.join(sumission_dir,'CSV_from_Redcap')\n",
    "match_file_path = os.path.join(template_dir, 'NDA_REDCap_matches.xlsx')\n",
    "df_match_names = pd.ExcelFile(match_file_path)\n",
    "list_questionnaire_names = df_match_names.sheet_names\n",
    "print(f\"Tabs found: {list_questionnaire_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abbe22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "# Calculate interview_age in months from date_of_birth to interview_date\n",
    "# Age is rounded to chronological month: 15 days = 0 months, 16 days = 1 month\n",
    "from datetime import datetime\n",
    "\n",
    "# Calculate interview_age in months\n",
    "def calculate_age_months(dob_str, interview_date_str):\n",
    "    if pd.isna(dob_str) or pd.isna(interview_date_str):\n",
    "        return None\n",
    "    try:\n",
    "        # Parse dates (date_of_birth is in yyyy/mm/dd format)\n",
    "        dob = pd.to_datetime(dob_str, format='%Y/%m/%d')\n",
    "        interview_date = pd.to_datetime(interview_date_str)\n",
    "        \n",
    "        # Calculate total days difference\n",
    "        delta = interview_date - dob\n",
    "        total_days = delta.days\n",
    "        \n",
    "        # Calculate base months (years * 12 + month difference)\n",
    "        years = interview_date.year - dob.year\n",
    "        months = interview_date.month - dob.month\n",
    "        base_months = years * 12 + months\n",
    "        \n",
    "        # Adjust based on day of month\n",
    "        # If interview day is earlier than birth day, we're in the previous month\n",
    "        if interview_date.day < dob.day:\n",
    "            base_months -= 1\n",
    "            # Calculate days into the current month\n",
    "            days_into_month = interview_date.day\n",
    "        else:\n",
    "            # Calculate days into the current month from birth day\n",
    "            days_into_month = interview_date.day - dob.day\n",
    "        \n",
    "        # Apply rounding rule: 15 days = 0 months, 16 days = 1 month\n",
    "        # If days_into_month >= 16, add 1 month\n",
    "        if days_into_month >= 16:\n",
    "            base_months += 1\n",
    "        \n",
    "        return max(0, base_months)  # Ensure non-negative\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Dictionary for phenotype_description mapping\n",
    "dic_phenotype_description = {\n",
    "    'HC': 'Matched healthy controls',\n",
    "    'SAD': 'Patients with Anxiety Disorders',\n",
    "    'BDD': 'Patients with Body Dysmorphic Disorder disorders',\n",
    "    'SAD/BDD': 'Patients with Anxiety Disorders and Body Dysmorphic Disorder disorders'\n",
    "}\n",
    "\n",
    "# Dictionary for Race 1=American Indian/Alaska Native; 2=Asian; 3=Black or African American; 4=awaiian or Pacific Islander; 5=White; 6=More than one race; 7=Unknown or not reported\n",
    "dic_race = {\n",
    "    1: \"American Indian/Alaska Native\",\n",
    "    2: \"Asian\",\n",
    "    3: \"Black or African American\",\n",
    "    4: \"Hawaiian or Pacific Islander\",\n",
    "    5: \"White\",\n",
    "    6: \"More than one race\",\n",
    "    7: \"Unknown or not reported\"\n",
    "}\n",
    "\n",
    "dic_mdd_lp = {\n",
    "    1:0,\n",
    "    3:1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e92e0151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/557835634.py:57: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#1. ndar_subject01\n",
    "#NDA: phenotype_description: HC=Matched healthy controls; SAD=Patients with anxiety disorders; BDD=Patients with Body Dysmorphic Disorder disorders\n",
    "#interview_age: Age in months (Age is rounded to chronological month. If the research participant is 15-days-old at time of interview, the appropriate value would be 0 months. If the participant is 16-days-old, the value would be 1 month.)\n",
    "#twins_study = 'NO'\n",
    "#sibling_study = 'NO'\n",
    "#family_study = 'NO'\n",
    "#sample_taken = 'NO'\n",
    "#M = Male; F = Female; O=Other; NR = Not reported\n",
    "questionaire_name = 'ndar_subject01'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "df['sex'] = df['sex'].fillna('NR')\n",
    "# Merge GROUP from df_meta to df['phenotype'] based on matching ID\n",
    "dob_map = df_meta.set_index('ID')['GROUP'].to_dict()\n",
    "df['phenotype'] = df['src_subject_id'].map(dob_map)\n",
    "# Assign phenotype_description based on phenotype using dictionary\n",
    "df['phenotype_description'] = df['phenotype'].map(dic_phenotype_description)\n",
    "# Assign race based on race using dictionary\n",
    "df['race'] = df['race'].map(dic_race)\n",
    "# Set fixed values for study type fields\n",
    "df['twins_study'] = 'No'\n",
    "df['sibling_study'] = 'No'\n",
    "df['family_study'] = 'No'\n",
    "df['sample_taken'] = 'No'\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be18ec22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/2321818529.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#2. cde_gad701\n",
    "#NDA:  0 = Not at all; 1 = Several days; 2= More than half the days; 3 = Nearly every day; -9 = Missing.\n",
    "#gad7_8: In what language did you collect the data? set to 1(English)\n",
    "questionaire_name = 'cde_gad701'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "df = df.dropna(subset=['subjectkey'])\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "df['sex'] = df['sex'].fillna('NR')\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "df = df.dropna(subset=['interview_age'])\n",
    "# Set gad7_8 to 1 as english\n",
    "df['gad7_8'] = '1'\n",
    "# Set GAD7's missing value to -9\n",
    "gad7_cols = ['gad7_1', 'gad7_2', 'gad7_3', 'gad7_4', 'gad7_5', 'gad7_6', 'gad7_7']\n",
    "df[gad7_cols] = df[gad7_cols].fillna(-9)\n",
    "df[gad7_cols] = df[gad7_cols].astype('Int64')\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f02ee9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/2071645460.py:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#3. cde_phq901\n",
    "#NDA: 0 = Not at all; 1 = Several days; 2 = More than half the days; 3 = Nearly every day; -9 = Missing\n",
    "#phq9_10: In what language did you collect the data? set to 1(English)\n",
    "questionaire_name = 'cde_phq901'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "df = df.dropna(subset=['subjectkey'])\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "df = df.dropna(subset=['interview_age'])\n",
    "# Set phq9_10 to 1 as english\n",
    "df['phq9_10'] = '1'\n",
    "# Set PHQ9's missing value to -9\n",
    "phq9_cols = ['phq9_1', 'phq9_2', 'phq9_3', 'phq9_4', 'phq9_5', 'phq9_6', 'phq9_7', 'phq9_8', 'phq9_9']\n",
    "df[phq9_cols] = df[phq9_cols].fillna(-9)\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "df[phq9_cols] = df[phq9_cols].astype('Int64')\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30098969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectkey</th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>interview_age</th>\n",
       "      <th>interview_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>phq9_1</th>\n",
       "      <th>phq9_2</th>\n",
       "      <th>phq9_3</th>\n",
       "      <th>phq9_4</th>\n",
       "      <th>phq9_5</th>\n",
       "      <th>phq9_6</th>\n",
       "      <th>phq9_7</th>\n",
       "      <th>phq9_8</th>\n",
       "      <th>phq9_9</th>\n",
       "      <th>phq9_10</th>\n",
       "      <th>study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cde_phq9</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INVMY773RAB</td>\n",
       "      <td>301</td>\n",
       "      <td>252</td>\n",
       "      <td>10/24/2024</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>baseline_1_arm_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INVNN813ZMK</td>\n",
       "      <td>302</td>\n",
       "      <td>235</td>\n",
       "      <td>10/25/2024</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>baseline_1_arm_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INVXW311FN4</td>\n",
       "      <td>303</td>\n",
       "      <td>217</td>\n",
       "      <td>10/28/2024</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>baseline_1_arm_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDAR_INVGM360PT8</td>\n",
       "      <td>305</td>\n",
       "      <td>257</td>\n",
       "      <td>11/05/2024</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>baseline_1_arm_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>NDARRR532GW3</td>\n",
       "      <td>136</td>\n",
       "      <td>300</td>\n",
       "      <td>12/03/2025</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>treatment_session_arm_1c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>NDARAN078FHK</td>\n",
       "      <td>127</td>\n",
       "      <td>259</td>\n",
       "      <td>12/05/2025</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>treatment_session_arm_1i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>NDARAN078FHK</td>\n",
       "      <td>127</td>\n",
       "      <td>259</td>\n",
       "      <td>12/10/2025</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>treatment_session_arm_1j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>NDARAN078FHK</td>\n",
       "      <td>127</td>\n",
       "      <td>260</td>\n",
       "      <td>12/16/2025</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>treatment_session_arm_1k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>NDARRR532GW3</td>\n",
       "      <td>136</td>\n",
       "      <td>301</td>\n",
       "      <td>01/08/2026</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>treatment_session_arm_1b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           subjectkey src_subject_id interview_age interview_date sex phq9_1  \\\n",
       "0            cde_phq9              1                                           \n",
       "1    NDAR_INVMY773RAB            301           252     10/24/2024   F      0   \n",
       "2    NDAR_INVNN813ZMK            302           235     10/25/2024   F      0   \n",
       "3    NDAR_INVXW311FN4            303           217     10/28/2024   M      0   \n",
       "4    NDAR_INVGM360PT8            305           257     11/05/2024   F      0   \n",
       "..                ...            ...           ...            ...  ..    ...   \n",
       "311      NDARRR532GW3            136           300     12/03/2025   F      0   \n",
       "312      NDARAN078FHK            127           259     12/05/2025   M      0   \n",
       "313      NDARAN078FHK            127           259     12/10/2025   M      0   \n",
       "314      NDARAN078FHK            127           260     12/16/2025   M      0   \n",
       "315      NDARRR532GW3            136           301     01/08/2026   F      0   \n",
       "\n",
       "    phq9_2 phq9_3 phq9_4 phq9_5 phq9_6 phq9_7 phq9_8 phq9_9 phq9_10  \\\n",
       "0                                                                     \n",
       "1        0      1      0      1      0      0      0      0       1   \n",
       "2        0      0      0      0      0      0      0      0       1   \n",
       "3        0      0      1      0      1      0      0      0       1   \n",
       "4        0      0      1      0      0      0      0      0       1   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...     ...   \n",
       "311      0      1      0      0      0      0      0      0       1   \n",
       "312      0      0      0      0      0      0      0      0       1   \n",
       "313      0      0      0      0      0      0      0      0       1   \n",
       "314      0      0      0      0      0      0      0      0       1   \n",
       "315      0      0      0      0      0      0      0      0       1   \n",
       "\n",
       "                        study  \n",
       "0                              \n",
       "1            baseline_1_arm_2  \n",
       "2            baseline_1_arm_2  \n",
       "3            baseline_1_arm_2  \n",
       "4            baseline_1_arm_2  \n",
       "..                        ...  \n",
       "311  treatment_session_arm_1c  \n",
       "312  treatment_session_arm_1i  \n",
       "313  treatment_session_arm_1j  \n",
       "314  treatment_session_arm_1k  \n",
       "315  treatment_session_arm_1b  \n",
       "\n",
       "[316 rows x 16 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06ebc076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/437738121.py:46: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#4. cde_dsm5crossad01\n",
    "#NDA: 0 = Not at all; 1 = Several days; 2 = More than half the days; 3 = Nearly every day; -9 = Missing\n",
    "#dsm5crossad_24: In what language did you collect the data? set to 1(English)\n",
    "questionaire_name = 'cde_dsm5crossad01'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Set dsm5crossad_24 to 1 as english\n",
    "df['dsm5crossad_24'] = '1'\n",
    "# Set PHQ9's missing value to -9\n",
    "dsm_cols = [f'dsm5crossad_{i}' for i in range(1, 24)]\n",
    "df[dsm_cols] = df[dsm_cols].fillna(-9)\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "df[dsm_cols] = df[dsm_cols].astype('Int64')\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76d80967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/3634995411.py:46: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#5. cde_whodas01\n",
    "#NDA: 0=None; 1=Mild; 2=Moderate; 3=Severe; 4=Extreme or cannot do; -9 = Missing\n",
    "#whodas_13: In what language did you collect the data? set to 1(English)\n",
    "questionaire_name = 'cde_whodas01'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Set whodas_13 to 1 as english\n",
    "df['whodas_13'] = '1'\n",
    "# Set cde_whodas01's missing value to -9\n",
    "dsm_cols = [f'whodas_{i}' for i in range(1, 13)]\n",
    "df[dsm_cols] = df[dsm_cols].fillna(-9)\n",
    "df[dsm_cols] = df[dsm_cols].astype('Int64')\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84b3811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/4014138881.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['bodydysmorphiclp'] = df['bodydysmorphiclp'].fillna(-9)\n",
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/4014138881.py:56: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['scid_p88'] = df['scid_p88'].fillna(555)\n",
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/4014138881.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#1. SCID \n",
    "#scidv_rv01 for now\n",
    "#scidv_mdd_lp: Major Depressive Disorders - Lifetime Prevalence; 0=Absent; 1=Present; -99 = Missing\n",
    "#bodydysmorphiclp: Body Dysmorphic Lifetime Prevalence; 1 = absent; 2 = sub-threshold; 3 = threshold; -9 = no data\n",
    "#scid_p88: Body Dysmorphic Disorder: Past Month; 1 = absent ; 3 = confirmed ; 555= Missing\n",
    "#scid_sad_lifetime: Social Anxiety Disorder - Lifetime; 0 = Unknown/Inadequate Information; 1 = Absent; 2 = Sub-threshold; 3 = Threshold\n",
    "#scid_gad_lifetime: Generalized Anxiety Disorder - Lifetime; 0 = Unknown/Inadequate Information; 1 = Absent; 2 = Sub-threshold; 3 = Threshold\n",
    "questionaire_name = 'scidv_rv01'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "\n",
    "# Find file paths\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "defination_path = [f for f in glob.glob(os.path.join(defination_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "\n",
    "# Load DataFrames\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "\n",
    "# Load Mapping\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "\n",
    "# Apply Mappings\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template'] \n",
    "    redcap_col = row['REDCap']     \n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "\n",
    "# Merge GUID and Basic Info\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "df['scidv_mdd_lp'] = df['scidv_mdd_lp'].map(dic_mdd_lp)\n",
    "\n",
    "# --- Manual Fixes (Apply these BEFORE type enforcement) ---\n",
    "# Set scidv_mdd_lp missing value to -99\n",
    "df['scidv_mdd_lp'] = df['scidv_mdd_lp'].fillna(-99)\n",
    "# Set bodydysmorphiclp missing value to -9\n",
    "df['bodydysmorphiclp'] = df['bodydysmorphiclp'].fillna(-9)\n",
    "# Set scid_p88 missing value to 555\n",
    "df['scid_p88'] = df['scid_p88'].fillna(555)\n",
    "\n",
    "\n",
    "# --- DYNAMIC TYPE ENFORCEMENT ---\n",
    "# 1. Load the definition file\n",
    "df_def = pd.read_csv(defination_path)\n",
    "dtype_map = dict(zip(df_def['ElementName'], df_def['DataType']))\n",
    "\n",
    "# 2. Iterate and apply types\n",
    "for col in df.columns:\n",
    "    if col in dtype_map:\n",
    "        target_type = str(dtype_map[col])\n",
    "        if 'Integer' in target_type:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "        elif 'Float' in target_type:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "\n",
    "# Drop rows missing critical info\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "\n",
    "# Format Date\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "# Trim columns to match template\n",
    "df = df[df_template.columns.tolist()]\n",
    "\n",
    "# --- CRITICAL FIX START ---\n",
    "# Convert to object to allow mixing Strings (Header) and Integers (Data)\n",
    "# This preserves the \"Int64\" formatting (no decimals) but allows the concat below to work.\n",
    "df = df.astype(object)\n",
    "\n",
    "# Replace 'pd.NA' (from Int64) with None/Empty string so CSV doesn't print '<NA>'\n",
    "df = df.where(pd.notnull(df), '')\n",
    "# --- CRITICAL FIX END ---\n",
    "\n",
    "# Add the first row (headers) from template\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "\n",
    "# Now this fillna will work because the column is type 'object', not 'Int64'\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "\n",
    "# Write to CSV\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c28a3594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/1615961507.py:49: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#2. lsas01_sr\n",
    "# #NDA: \n",
    "#fear 0::3;-7; -9\t0=None; 1=Mild; 2=Moderate; 3=Severe; -7; -9=incomplete\n",
    "#avoidence 0=Never; 1=Occasionally; 2=Often; 3=Usually; 4=Almost always; -7; -9=Incomplete\n",
    "questionaire_name = 'lsas01_sr'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "# Set lsps01_sr's missing value to -9\n",
    "start_col = 'phonpubf'\n",
    "end_col = 'resista'\n",
    "fill_val = -9\n",
    "cols_to_fix = df.loc[:, start_col:end_col].columns\n",
    "df[cols_to_fix] = df[cols_to_fix].fillna(fill_val)\n",
    "df[cols_to_fix] = df[cols_to_fix].astype('Int64')\n",
    "\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bde872b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/1615961507.py:49: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#2. lsas01_sr\n",
    "# #NDA: \n",
    "#fear 0::3;-7; -9\t0=None; 1=Mild; 2=Moderate; 3=Severe; -7; -9=incomplete\n",
    "#avoidence 0=Never; 1=Occasionally; 2=Often; 3=Usually; 4=Almost always; -7; -9=Incomplete\n",
    "questionaire_name = 'lsas01_sr'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "# Set lsps01_sr's missing value to -9\n",
    "start_col = 'phonpubf'\n",
    "end_col = 'resista'\n",
    "fill_val = -9\n",
    "cols_to_fix = df.loc[:, start_col:end_col].columns\n",
    "df[cols_to_fix] = df[cols_to_fix].fillna(fill_val)\n",
    "df[cols_to_fix] = df[cols_to_fix].astype('Int64')\n",
    "\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2488e744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/3779840579.py:49: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#2. lsas01_cr\n",
    "# #NDA: \n",
    "#fear 0::3;-7; -9\t0=None; 1=Mild; 2=Moderate; 3=Severe; -7; -9=incomplete\n",
    "#avoidence 0=Never; 1=Occasionally; 2=Often; 3=Usually; 4=Almost always; -7; -9=Incomplete\n",
    "questionaire_name = 'lsas01_cr'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "# Set lsps01_sr's missing value to -9\n",
    "start_col = 'phonpubf'\n",
    "end_col = 'resista'\n",
    "fill_val = -9\n",
    "cols_to_fix = df.loc[:, start_col:end_col].columns\n",
    "df[cols_to_fix] = df[cols_to_fix].fillna(fill_val)\n",
    "df[cols_to_fix] = df[cols_to_fix].astype('Int64')\n",
    "\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96756c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectkey</th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>interview_date</th>\n",
       "      <th>interview_age</th>\n",
       "      <th>sex</th>\n",
       "      <th>dayscons</th>\n",
       "      <th>visit</th>\n",
       "      <th>certcode</th>\n",
       "      <th>phonpubf</th>\n",
       "      <th>phonpuba</th>\n",
       "      <th>...</th>\n",
       "      <th>aasocial_anx</th>\n",
       "      <th>aasocial_avo</th>\n",
       "      <th>aaperform_anx</th>\n",
       "      <th>aaperform_avo</th>\n",
       "      <th>totalanx</th>\n",
       "      <th>totalav</th>\n",
       "      <th>version_form</th>\n",
       "      <th>askqs</th>\n",
       "      <th>askqas</th>\n",
       "      <th>timepoint_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lsps</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INVNZ644JB4</td>\n",
       "      <td>102</td>\n",
       "      <td>01/09/2025</td>\n",
       "      <td>241</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>baseline_1_arm_1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>baseline_1_arm_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INVNZ644JB4</td>\n",
       "      <td>102</td>\n",
       "      <td>01/27/2025</td>\n",
       "      <td>242</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>repeat_baseline_arm_1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>repeat_baseline_arm_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INVNZ644JB4</td>\n",
       "      <td>102</td>\n",
       "      <td>02/26/2025</td>\n",
       "      <td>243</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>treatment_session_arm_1d</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>treatment_session_arm_1d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDAR_INVNZ644JB4</td>\n",
       "      <td>102</td>\n",
       "      <td>03/17/2025</td>\n",
       "      <td>244</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>treatment_session_arm_1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>treatment_session_arm_1h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>NDARZX442CRX</td>\n",
       "      <td>134</td>\n",
       "      <td>12/16/2025</td>\n",
       "      <td>251</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>treatment_session_arm_1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>treatment_session_arm_1h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>NDARLB096RHM</td>\n",
       "      <td>135</td>\n",
       "      <td>10/23/2025</td>\n",
       "      <td>253</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>baseline_1_arm_1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>baseline_1_arm_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>NDARLB096RHM</td>\n",
       "      <td>135</td>\n",
       "      <td>11/06/2025</td>\n",
       "      <td>253</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>repeat_baseline_arm_1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>repeat_baseline_arm_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>NDARLB096RHM</td>\n",
       "      <td>135</td>\n",
       "      <td>12/04/2025</td>\n",
       "      <td>254</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>treatment_session_arm_1d</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>treatment_session_arm_1d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>NDARLB096RHM</td>\n",
       "      <td>135</td>\n",
       "      <td>12/15/2025</td>\n",
       "      <td>254</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>treatment_session_arm_1h</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>treatment_session_arm_1h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subjectkey src_subject_id interview_date interview_age sex dayscons  \\\n",
       "0               lsps              1                                             \n",
       "1   NDAR_INVNZ644JB4            102     01/09/2025           241   F     None   \n",
       "2   NDAR_INVNZ644JB4            102     01/27/2025           242   F     None   \n",
       "3   NDAR_INVNZ644JB4            102     02/26/2025           243   F     None   \n",
       "4   NDAR_INVNZ644JB4            102     03/17/2025           244   F     None   \n",
       "..               ...            ...            ...           ...  ..      ...   \n",
       "81      NDARZX442CRX            134     12/16/2025           251   M     None   \n",
       "82      NDARLB096RHM            135     10/23/2025           253   F     None   \n",
       "83      NDARLB096RHM            135     11/06/2025           253   F     None   \n",
       "84      NDARLB096RHM            135     12/04/2025           254   F     None   \n",
       "85      NDARLB096RHM            135     12/15/2025           254   F     None   \n",
       "\n",
       "                       visit certcode phonpubf phonpuba  ... aasocial_anx  \\\n",
       "0                                                        ...                \n",
       "1           baseline_1_arm_1     None        0        0  ...         None   \n",
       "2      repeat_baseline_arm_1     None        1        0  ...         None   \n",
       "3   treatment_session_arm_1d     None        1        0  ...         None   \n",
       "4   treatment_session_arm_1h     None        1        0  ...         None   \n",
       "..                       ...      ...      ...      ...  ...          ...   \n",
       "81  treatment_session_arm_1h     None        2        3  ...         None   \n",
       "82          baseline_1_arm_1     None        0        0  ...         None   \n",
       "83     repeat_baseline_arm_1     None        0        0  ...         None   \n",
       "84  treatment_session_arm_1d     None        0        0  ...         None   \n",
       "85  treatment_session_arm_1h     None        0        0  ...         None   \n",
       "\n",
       "   aasocial_avo aaperform_anx aaperform_avo totalanx totalav version_form  \\\n",
       "0                                                                           \n",
       "1          None          None          None     None    None         None   \n",
       "2          None          None          None     None    None         None   \n",
       "3          None          None          None     None    None         None   \n",
       "4          None          None          None     None    None         None   \n",
       "..          ...           ...           ...      ...     ...          ...   \n",
       "81         None          None          None     None    None         None   \n",
       "82         None          None          None     None    None         None   \n",
       "83         None          None          None     None    None         None   \n",
       "84         None          None          None     None    None         None   \n",
       "85         None          None          None     None    None         None   \n",
       "\n",
       "   askqs askqas           timepoint_label  \n",
       "0                                          \n",
       "1   None   None          baseline_1_arm_1  \n",
       "2   None   None     repeat_baseline_arm_1  \n",
       "3   None   None  treatment_session_arm_1d  \n",
       "4   None   None  treatment_session_arm_1h  \n",
       "..   ...    ...                       ...  \n",
       "81  None   None  treatment_session_arm_1h  \n",
       "82  None   None          baseline_1_arm_1  \n",
       "83  None   None     repeat_baseline_arm_1  \n",
       "84  None   None  treatment_session_arm_1d  \n",
       "85  None   None  treatment_session_arm_1h  \n",
       "\n",
       "[86 rows x 88 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f116115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/4063545434.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#3. sds01\n",
    "#NDA: 0=not at all; 1-3=mildly; 4-6=moderately; 7-9=markedly; 10=extremely; -7= I have not worked/studied at all during the past week for reasons unrelated to the disorder\n",
    "questionaire_name = 'sds01'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab43eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3aec77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/2744229511.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#5. qids01\n",
    "#NDA: 0::3; 999=missing\n",
    "questionaire_name = 'qids01'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "# Set lsps01_sr's missing value to -9\n",
    "start_col = 'vsoin'\n",
    "end_col = 'vagit'\n",
    "fill_val = 999\n",
    "cols_to_fix = df.loc[:, start_col:end_col].columns\n",
    "df[cols_to_fix] = df[cols_to_fix].fillna(fill_val)\n",
    "df[cols_to_fix] = df[cols_to_fix].astype('Int64')\n",
    "df['qvtot'] = df['qvtot'].astype('Int64')\n",
    "\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e26a8da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/3314267707.py:50: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#6. pswq01\n",
    "#NDA: 5= Not at all typical of me; 1= Very typical of me\n",
    "#pswq_total, 99\n",
    "#110\ttreatment_session_arm_1f\n",
    "questionaire_name = 'pswq01'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "# Set lsps01_sr's missing value to -9\n",
    "start_col = 'pswq1'\n",
    "end_col = 'pswq_total'\n",
    "fill_val = 99\n",
    "cols_to_fix = df.loc[:, start_col:end_col].columns\n",
    "df[cols_to_fix] = df[cols_to_fix].fillna(fill_val)\n",
    "df[cols_to_fix] = df[cols_to_fix].astype('Int64')\n",
    "df['pswq_total'] = df['pswq_total'].astype('float')\n",
    "\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f200d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/1028507069.py:45: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#7. hars01\n",
    "#NDA: 0::4\n",
    "questionaire_name = 'hars01'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "start_col = 'ham_a_q1'\n",
    "end_col = 'ham_a_q14'\n",
    "cols_to_fix = df.loc[:, start_col:end_col].columns\n",
    "df[cols_to_fix] = df[cols_to_fix].astype('Int64')\n",
    "\n",
    "\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aaac8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. babs01\n",
    "#NDA: 0::4\n",
    "#babs_total Sum of Questions 1 - 6\n",
    "questionaire_name = 'babs01'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "df['idor'] = df['idor'].astype('Int64')\n",
    "\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed21c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/4030267616.py:44: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#9. rumination01\n",
    "#NDA: 1 = Almost never; 2 = Sometimes; 3 = Often; 4 = Almost always\n",
    "questionaire_name = 'rumination01'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "df_target_match_names = df_match_names.parse(questionaire_name, usecols=[0, 1])\n",
    "valid_mappings = df_target_match_names.dropna(subset=['REDCap'])\n",
    "df = df_template.copy() \n",
    "for index, row in valid_mappings.iterrows():\n",
    "    nda_col = row['NDA_template']  # The destination column name\n",
    "    redcap_col = row['REDCap']     # The source column name\n",
    "    if redcap_col in df_redcap.columns:\n",
    "        df[nda_col] = df_redcap[redcap_col]\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "start_col = 'rumination01'\n",
    "end_col = 'rumination_total'\n",
    "cols_to_fix = df.loc[:, start_col:end_col].columns\n",
    "df[cols_to_fix] = df[cols_to_fix].astype('Int64')\n",
    "\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1d7ff6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/3j_hstl96w58qx1sdw9czhxr0000gn/T/ipykernel_54878/3464242018.py:83: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['interview_date'] = pd.to_datetime(df['interview_date'])\n"
     ]
    }
   ],
   "source": [
    "#10. image03\n",
    "questionaire_name = 'image03'\n",
    "results_file = os.path.join(result_dir, f'{questionaire_name}.csv')\n",
    "template_path = [f for f in glob.glob(os.path.join(approved_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "defination_path = [f for f in glob.glob(os.path.join(defination_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_def = pd.read_csv(defination_path)\n",
    "dtype_map = dict(zip(df_def['ElementName'], df_def['DataType']))\n",
    "redcap_file_path = [f for f in glob.glob(os.path.join(redcap_file_dir, '*.csv')) if questionaire_name.lower() in os.path.basename(f).lower()][0]\n",
    "df_redcap = pd.read_csv(redcap_file_path)\n",
    "extract_file = os.path.join('/Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/', 'NDA_image03_extracted.csv')\n",
    "df_extract = pd.read_csv(extract_file)\n",
    "df_template = pd.read_csv(template_path, header=1)\n",
    "# Read the first row of template_path with all columns\n",
    "first_row_template = pd.read_csv(template_path, header=None, nrows=1, names=df_template.columns)\n",
    "df = df_template.copy() \n",
    "common_cols = df_template.columns.intersection(df_extract.columns)\n",
    "df = df_extract[common_cols].copy()\n",
    "# Merge GUID from df_meta to df['subjectkey'] based on matching ID\n",
    "guid_map = df_meta.set_index('ID')['GUID'].to_dict()\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df['subjectkey'] = df['src_subject_id'].map(guid_map)\n",
    "# 1. Ensure IDs are strings for reliable matching\n",
    "df['src_subject_id'] = df['src_subject_id'].astype(str).str.strip()\n",
    "df_redcap['record_id'] = df_redcap['record_id'].astype(str).str.strip()\n",
    "\n",
    "# 2. Optimization: Group REDCap data by ID\n",
    "redcap_grouped = df_redcap.groupby('record_id')\n",
    "\n",
    "def get_mri_date(row):\n",
    "    subject_id = row['src_subject_id']\n",
    "    \n",
    "    # Handle NaN visits gracefully and convert to lower case\n",
    "    visit_val = row['visit']\n",
    "    if pd.isna(visit_val):\n",
    "        return row['interview_date']\n",
    "    \n",
    "    visit_prefix = str(visit_val).lower()\n",
    "    \n",
    "    # Check if this subject exists in REDCap\n",
    "    if subject_id in redcap_grouped.groups:\n",
    "        subj_data = redcap_grouped.get_group(subject_id)\n",
    "        \n",
    "        # LOGIC: Check startswith ignoring case\n",
    "        # We convert the entire column to lower(), then check startswith\n",
    "        match = subj_data[\n",
    "            subj_data['redcap_event_name']\n",
    "            .fillna('')\n",
    "            .str.lower()\n",
    "            .str.startswith(visit_prefix)\n",
    "        ]\n",
    "        \n",
    "        # If match found, return the mri_exit_date\n",
    "        if not match.empty:\n",
    "            # Return the first matching date (or handle duplicates if needed)\n",
    "            return match.iloc[0]['mri_exit_date']\n",
    "            \n",
    "    return row['interview_date']\n",
    "\n",
    "# 3. Apply the function\n",
    "df['interview_date'] = df.apply(get_mri_date, axis=1)\n",
    "# Merge SEX from df_meta to df['sex'] based on matching ID\n",
    "sex_map = df_meta.set_index('ID')['SEX'].to_dict()\n",
    "df['sex'] = df['src_subject_id'].map(sex_map)\n",
    "# Merge date_of_birth from df_meta into df\n",
    "dob_map = df_meta.set_index('ID')['date_of_birth'].to_dict()\n",
    "df['date_of_birth'] = df['src_subject_id'].map(dob_map)\n",
    "df['interview_age'] = df.apply(\n",
    "    lambda row: calculate_age_months(row['date_of_birth'], row['interview_date']), \n",
    "    axis=1\n",
    ")\n",
    "# Convert interview_age to integer\n",
    "df['interview_age'] = df['interview_age'].astype('Int64')\n",
    "df['experiment_id'] = df['experiment_id'].astype('Int64')\n",
    "df['image_extent4'] = df['image_extent4'].astype('Int64')\n",
    "\n",
    "\n",
    "df = df.astype(object)\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df = df.dropna(subset=['interview_age', 'interview_date'])\n",
    "df = df[~df['scan_type'].astype(str).str.contains('localizer scan', case=False, na=False)].copy()\n",
    "df = df[~df['scan_type'].astype(str).str.contains('MR structural (T2)', case=False, na=False, regex=False)].copy()\n",
    "# Convert interview_date to 'MM/DD/YYYY' string format\n",
    "df['interview_date'] = pd.to_datetime(df['interview_date'])\n",
    "df['interview_date'] = df['interview_date'].dt.strftime('%m/%d/%Y')\n",
    "# Trim df to only have columns as in df_template, in the exact same order\n",
    "df = df[df_template.columns.tolist()]\n",
    "# Add the first row from template_path to df\n",
    "df = pd.concat([first_row_template, df], ignore_index=True)\n",
    "df.iloc[0] = df.iloc[0].fillna('')\n",
    "df.iloc[:1].to_csv(results_file, index=False, header=False, mode='w')\n",
    "df.iloc[1:].to_csv(results_file, index=False, header=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "addaa82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['src_subject_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d19dcfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING CLEANUP (Dry Run: True) ---\n",
      "Scanning: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image\n",
      "\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-302/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-302/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-302/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-130/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-130/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-130/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-305/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-305/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-305/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-108/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-108/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-108/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-108/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-106/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-106/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-106/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-106/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-304/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-304/ses-baseline1/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-304/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-136/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-136/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-131/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-303/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-303/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-303/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-113/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-113/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-113/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-113/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-319/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-319/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-319/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-326/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-326/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-310/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-310/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-310/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-328/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-317/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-317/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-317/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-125/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-125/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-125/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-125/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-329/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-329/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-124/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-124/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-124/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-124/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-311/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-311/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-311/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-123/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-123/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-123/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-123/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-318/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-318/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-327/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-327/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-320/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-320/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-320/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-134/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-133/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-301/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-301/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-301/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-105/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-105/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-105/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-105/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-308/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-308/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-308/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-330/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-102/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-102/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-102/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-102/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-103/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-331/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-104/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-104/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-104/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-104/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-309/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-309/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-309/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-132/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-132/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-132/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-135/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-307/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-307/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-307/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-325/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-325/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-117/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-117/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-117/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-117/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-110/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-110/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-110/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-119/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-119/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-119/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-119/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-126/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-126/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-314/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-314/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-314/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-313/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-313/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-313/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-121/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-121/ses-T6/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-121/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-121/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-312/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-312/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-312/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-118/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-315/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-315/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-315/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-127/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-127/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-323/ses-T12/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-323/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-323/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-111/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-111/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-324/ses-repeatbaseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-324/ses-baseline/anat\n",
      "Checking folder: /Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image/sub-2001LH/ses-pilotTR1000/anat\n",
      "\n",
      "--- SUMMARY ---\n",
      "Anat folders processed: 141\n",
      "Files marked for deletion: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configuration\n",
    "root_dir = '/Users/xiaoqianxiao/lab/R01/NDA/sumission_20260115/final/image'\n",
    "target_subfolder = 'anat'\n",
    "target_tag = 'desc-defaced_T1w'\n",
    "\n",
    "# SET TO FALSE TO ACTUALLY DELETE\n",
    "dry_run = True\n",
    "#dry_run = False\n",
    "\n",
    "print(f\"--- STARTING CLEANUP (Dry Run: {dry_run}) ---\")\n",
    "print(f\"Scanning: {root_dir}\\n\")\n",
    "\n",
    "cleaned_folders = 0\n",
    "deleted_files = 0\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # Only process directories named 'anat'\n",
    "    if os.path.basename(dirpath) == target_subfolder:\n",
    "        cleaned_folders += 1\n",
    "        print(f\"Checking folder: {dirpath}\")\n",
    "        \n",
    "        # 1. Identify the \"Master\" files (the defaced NIfTIs)\n",
    "        # We look for files containing 'desc-defaced_T1w' ending in .nii or .nii.gz\n",
    "        defaced_niftis = [f for f in filenames if target_tag in f and (f.endswith('.nii') or f.endswith('.nii.gz'))]\n",
    "        \n",
    "        # 2. Build the Whitelist (Files we MUST keep)\n",
    "        whitelist = set()\n",
    "        \n",
    "        for nifti in defaced_niftis:\n",
    "            # Add the image itself to whitelist\n",
    "            whitelist.add(nifti)\n",
    "            \n",
    "            # Derive the corresponding JSON name\n",
    "            # Logic: Remove 'desc-defaced_' and switch extension to .json\n",
    "            # e.g., sub-102_..._desc-defaced_T1w.nii.gz -> sub-102_..._T1w.json\n",
    "            \n",
    "            # Remove extension first\n",
    "            if nifti.endswith('.nii.gz'):\n",
    "                stem = nifti[:-7]\n",
    "            else:\n",
    "                stem = nifti[:-4]\n",
    "            \n",
    "            # Remove the 'desc-defaced_' tag to match the original JSON\n",
    "            # We use replace() to handle cases where it might be in the middle\n",
    "            json_stem = stem.replace('desc-defaced_', '') \n",
    "            json_filename = json_stem + '.json'\n",
    "            \n",
    "            # If this JSON actually exists, add it to whitelist\n",
    "            if json_filename in filenames:\n",
    "                whitelist.add(json_filename)\n",
    "        \n",
    "        # 3. Delete everything NOT in the whitelist\n",
    "        for file in filenames:\n",
    "            if file not in whitelist:\n",
    "                full_path = os.path.join(dirpath, file)\n",
    "                \n",
    "                if dry_run:\n",
    "                    print(f\"  [WOULD DELETE]: {file}\")\n",
    "                else:\n",
    "                    os.remove(full_path)\n",
    "                    print(f\"  [DELETED]: {file}\")\n",
    "                deleted_files += 1\n",
    "            else:\n",
    "                # Optional: print what we are keeping just to be sure\n",
    "                # print(f\"  [KEEPING]: {file}\")\n",
    "                pass\n",
    "\n",
    "print(f\"\\n--- SUMMARY ---\")\n",
    "print(f\"Anat folders processed: {cleaned_folders}\")\n",
    "print(f\"Files {'marked for deletion' if dry_run else 'deleted'}: {deleted_files}\")\n",
    "\n",
    "if dry_run and deleted_files > 0:\n",
    "    print(\">>> Logic look good? Set 'dry_run = False' and run again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a92a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
